{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE, SelectKBest, chi2, SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, make_scorer, average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# change the names accordingly\n",
    "df = pd.read_csv(\"SDSS_processed.csv\")\n",
    "df = df.drop('z', axis=1)\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Preliminary Model Running \n",
    "\n",
    "Decision Tree, Random Forest, SVM, KNN, neural networks, and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def metrics(y_test, y_pred, y_scores):\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    n_classes = len(np.unique(y_test))\n",
    "    precision_recall_auc = []\n",
    "    for i in range(n_classes):\n",
    "        y_test_binary = (y_test == i).astype(int)\n",
    "        y_scores_binary = y_scores[:, i]\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_test_binary, y_scores_binary)\n",
    "        auc_score = auc(recall_curve, precision_curve)\n",
    "        precision_recall_auc.append(auc_score)\n",
    "\n",
    "    average_precision_recall_auc = np.mean(precision_recall_auc)\n",
    "\n",
    "    return precision, recall, f1, average_precision_recall_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "precisions, recalls, f1s, auc_prs = [], [], [], []\n",
    "total_time = 0\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    start_time = time.time()\n",
    "\n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred_dt = dt.predict(X_test)\n",
    "    y_scores_dt = dt.predict_proba(X_test)\n",
    "\n",
    "    precision, recall, f1, auc_pr = metrics(y_test, y_pred_dt, y_scores_dt)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    auc_prs.append(auc_pr)\n",
    "\n",
    "    total_time += time.time() - start_time\n",
    "\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_f1 = np.mean(f1s)\n",
    "avg_auc_pr = np.mean(auc_prs)\n",
    "avg_time = total_time / n_runs\n",
    "\n",
    "avg_precision, avg_recall, avg_f1, avg_auc_pr, avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "precisions_rf, recalls_rf, f1s_rf, auc_prs_rf = [], [], [], []\n",
    "total_time_rf = 0\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    start_time_rf = time.time()\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    y_scores_rf = rf.predict_proba(X_test)\n",
    "\n",
    "    precision_rf, recall_rf, f1_rf, auc_pr_rf = metrics(y_test, y_pred_rf, y_scores_rf)\n",
    "    precisions_rf.append(precision_rf)\n",
    "    recalls_rf.append(recall_rf)\n",
    "    f1s_rf.append(f1_rf)\n",
    "    auc_prs_rf.append(auc_pr_rf)\n",
    "\n",
    "    total_time_rf += time.time() - start_time_rf\n",
    "\n",
    "avg_precision_rf = np.mean(precisions_rf)\n",
    "avg_recall_rf = np.mean(recalls_rf)\n",
    "avg_f1_rf = np.mean(f1s_rf)\n",
    "avg_auc_pr_rf = np.mean(auc_prs_rf)\n",
    "avg_time_rf = total_time_rf / n_runs\n",
    "\n",
    "avg_precision_rf, avg_recall_rf, avg_f1_rf, avg_auc_pr_rf, avg_time_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "precisions_svm, recalls_svm, f1s_svm, auc_prs_svm = [], [], [], []\n",
    "total_time_svm = 0\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    start_time_svm = time.time()\n",
    "\n",
    "    svm = CalibratedClassifierCV(SVC())\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    y_scores_svm = svm.predict_proba(X_test)\n",
    "\n",
    "    precision_svm, recall_svm, f1_svm, auc_pr_svm = metrics(y_test, y_pred_svm, y_scores_svm)\n",
    "    precisions_svm.append(precision_svm)\n",
    "    recalls_svm.append(recall_svm)\n",
    "    f1s_svm.append(f1_svm)\n",
    "    auc_prs_svm.append(auc_pr_svm)\n",
    "\n",
    "    total_time_svm += time.time() - start_time_svm\n",
    "\n",
    "avg_precision_svm = np.mean(precisions_svm)\n",
    "avg_recall_svm = np.mean(recalls_svm)\n",
    "avg_f1_svm = np.mean(f1s_svm)\n",
    "avg_auc_pr_svm = np.mean(auc_prs_svm)\n",
    "avg_time_svm = total_time_svm / n_runs\n",
    "\n",
    "avg_precision_svm, avg_recall_svm, avg_f1_svm, avg_auc_pr_svm, avg_time_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "precisions_knn, recalls_knn, f1s_knn, auc_prs_knn = [], [], [], []\n",
    "total_time_knn = 0\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X, y, test_size=0.3)\n",
    "    X_train_knn = X_train_knn.to_numpy()\n",
    "    X_test_knn = X_test_knn.to_numpy()\n",
    "    y_train_knn = y_train_knn.to_numpy()\n",
    "    y_test_knn = y_test_knn.to_numpy()\n",
    "\n",
    "    start_time_knn = time.time()\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train_knn, y_train_knn)\n",
    "    y_pred_knn = knn.predict(X_test_knn)\n",
    "    y_scores_knn = knn.predict_proba(X_test_knn)\n",
    "\n",
    "    precision_knn, recall_knn, f1_knn, auc_pr_knn = metrics(y_test_knn, y_pred_knn, y_scores_knn)\n",
    "    precisions_knn.append(precision_knn)\n",
    "    recalls_knn.append(recall_knn)\n",
    "    f1s_knn.append(f1_knn)\n",
    "    auc_prs_knn.append(auc_pr_knn)\n",
    "\n",
    "    total_time_knn += time.time() - start_time_knn\n",
    "\n",
    "avg_precision_knn = np.mean(precisions_knn)\n",
    "avg_recall_knn = np.mean(recalls_knn)\n",
    "avg_f1_knn = np.mean(f1s_knn)\n",
    "avg_auc_pr_knn = np.mean(auc_prs_knn)\n",
    "avg_time_knn = total_time_knn / n_runs\n",
    "\n",
    "avg_precision_knn, avg_recall_knn, avg_f1_knn, avg_auc_pr_knn, avg_time_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "precisions_xgb, recalls_xgb, f1s_xgb, auc_prs_xgb = [], [], [], []\n",
    "total_time_xgb = 0\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    start_time_xgb = time.time()\n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_train_xgb, y_train_xgb)\n",
    "    y_pred_xgb = xgb.predict(X_test_xgb)\n",
    "    y_scores_xgb = xgb.predict_proba(X_test_xgb)\n",
    "\n",
    "    precision_xgb, recall_xgb, f1_xgb, auc_pr_xgb = metrics(y_test_xgb, y_pred_xgb, y_scores_xgb)\n",
    "    precisions_xgb.append(precision_xgb)\n",
    "    recalls_xgb.append(recall_xgb)\n",
    "    f1s_xgb.append(f1_xgb)\n",
    "    auc_prs_xgb.append(auc_pr_xgb)\n",
    "\n",
    "    total_time_xgb += time.time() - start_time_xgb\n",
    "\n",
    "avg_precision_xgb = np.mean(precisions_xgb)\n",
    "avg_recall_xgb = np.mean(recalls_xgb)\n",
    "avg_f1_xgb = np.mean(f1s_xgb)\n",
    "avg_auc_pr_xgb = np.mean(auc_prs_xgb)\n",
    "avg_time_xgb = total_time_xgb / n_runs\n",
    "\n",
    "avg_precision_xgb, avg_recall_xgb, avg_f1_xgb, avg_auc_pr_xgb, avg_time_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auc_pr_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "tree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5, scoring=auc_pr_scorer, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best AUC-PR score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auc_pr_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='f1_weighted', verbose=20)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Best F1-weighted score for Random Forest:\", grid_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auc_pr_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "param_grid_svm = {\n",
    "    'base_estimator__C': [0.001, 0.1, 1, 10],\n",
    "    'base_estimator__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'base_estimator__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svm = CalibratedClassifierCV(SVC(), cv=3)\n",
    "grid_search_svm = GridSearchCV(svm, param_grid_svm, cv=5, scoring=auc_pr_scorer, verbose=1, n_jobs=-1)\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for SVM:\", grid_search_svm.best_params_)\n",
    "print(\"Best AUC-PR score for SVM:\", grid_search_svm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auc_pr_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring=auc_pr_scorer, verbose=1, n_jobs=-1)\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for KNN:\", grid_search_knn.best_params_)\n",
    "print(\"Best AUC-PR score for KNN:\", grid_search_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'max_depth': [None, 1, 5, 10],\n",
    "    'reg_alpha': [0, 0.1, 1, 10],\n",
    "    'reg_lambda': [0, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=3, scoring='f1_weighted', verbose=20)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for XGBoost:\", grid_search_xgb.best_params_)\n",
    "print(\"Best AUC-PR score for XGBoost:\", grid_search_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "feature_importances = np.zeros((num_runs, X.shape[1]))\n",
    "\n",
    "for i in range(num_runs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    model = XGBClassifier(learning_rate=0.1, max_depth=10, n_estimators=300, reg_alpha=1, reg_lambda=10,\n",
    "                              objective='multi:softmax', num_class=len(y.unique()))\n",
    "    model.fit(X_train, y_train)\n",
    "    feature_importances[i, :] = model.feature_importances_\n",
    "    print(i)\n",
    "\n",
    "avg_feature_importances = feature_importances.mean(axis=0)\n",
    "sorted_idx = avg_feature_importances.argsort()[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(range(14), avg_feature_importances[sorted_idx])\n",
    "plt.yticks(range(14), X.columns[sorted_idx])\n",
    "plt.xlabel('Average Feature Importance')\n",
    "plt.title('Features Importance for XGBoost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "feature_importances = np.zeros((num_runs, X_new.shape[1]))\n",
    "\n",
    "for i in range(num_runs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=i)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=10, \n",
    "                                min_samples_split=4, min_samples_leaf=2, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    feature_importances[i, :] = rf.feature_importances_\n",
    "    print(i)\n",
    "\n",
    "# Average feature importances across all runs\n",
    "avg_feature_importances_rf = feature_importances.mean(axis=0)\n",
    "sorted_idx_rf = avg_feature_importances.argsort()[::-1][:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx_rfavg_feature_importances_rf = feature_importances.mean(axis=0)\n",
    "sorted_idx_rf = sorted_idx_rfavg_feature_importances_rf.argsort()[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(range(9), avg_feature_importances_rf[sorted_idx_rf])\n",
    "plt.yticks(range(9), X_new.columns[sorted_idx_rf])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Features Importance for Random Forest')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_xgb = ['psfMag_u', 'psfMag_g', 'psfMag_r', 'psfMag_i', 'psfMag_z',\n",
    "                'petroMag_u', 'petroMag_g', 'petroMag_r', 'petroMag_i', 'petroMag_z',\n",
    "                'psf_u-g', 'psf_g-r', 'petro_u-g', 'petro_g-r']\n",
    "features_rf = ['psfMag_u', 'psfMag_g', 'psfMag_r', 'psfMag_i', 'psfMag_z',\n",
    "               'petroMag_r', 'petroMag_i', 'petroMag_z', 'psf_u-g']\n",
    "all_features = set(features_rf + features_xgb)\n",
    "combined_importances_rf = {feature: avg_feature_importances_rf[features_rf.index(feature)] if feature in features_rf else 0 \n",
    "                           for feature in all_features}\n",
    "combined_importances_xgb = {feature: avg_feature_importances[features_xgb.index(feature)] if feature in features_xgb else 0 \n",
    "                            for feature in all_features}\n",
    "\n",
    "# Sorting features based on the feature importance of XGBoost\n",
    "sorted_features_xgb = sorted(all_features, key=lambda x: combined_importances_xgb[x], reverse=True)\n",
    "\n",
    "# Creating arrays for plotting based on XGBoost sorted features\n",
    "importances_rf_plot = [combined_importances_rf[feature] for feature in sorted_features_xgb]\n",
    "importances_xgb_plot = [combined_importances_xgb[feature] for feature in sorted_features_xgb]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "indices = np.arange(len(sorted_features_xgb))\n",
    "\n",
    "plt.barh(indices - bar_width/2, importances_xgb_plot, bar_width, label='XGBoost', color='#377eb8')\n",
    "plt.barh(indices + bar_width/2, importances_rf_plot, bar_width, label='Random Forest', color='#4daf4a')\n",
    "\n",
    "plt.yticks(indices, sorted_features_xgb)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "xgb_model = XGBClassifier(learning_rate=0.1, max_depth=10, n_estimators=300, reg_aplha=1, reg_lambda=10)\n",
    "rfecv = RFECV(estimator=xgb_model, step=1, cv=3, scoring='f1_weighted', verbose=20)\n",
    "rfecv.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_rd = RandomForestClassifier(n_estimators=200, max_depth=10, \n",
    "                                  min_samples_split=4, min_samples_leaf=2)\n",
    "rfecv_rd = RFECV(estimator=rf_model_rd, step=1, cv=3, scoring='f1_weighted', verbose=20)\n",
    "rfecv_rd.fit(X, y)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "max_score_idx = np.argmax(rfecv.cv_results_['mean_test_score'])\n",
    "max_score = rfecv.cv_results_['mean_test_score'][max_score_idx]\n",
    "optimal_num_features = max_score_idx + 1 \n",
    "\n",
    "max_score_idx_rf = np.argmax(rfecv_rd.cv_results_['mean_test_score'])\n",
    "max_score_rf = rfecv_rd.cv_results_['mean_test_score'][max_score_idx_rf]\n",
    "optimal_num_features_rf = max_score_idx_rf + 1\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'], label='XGBoost', color='#377eb8')\n",
    "plt.plot(range(1, len(rfecv_rd.cv_results_['mean_test_score']) + 1), rfecv_rd.cv_results_['mean_test_score'], label='Random Forest', color='#4daf4a')\n",
    "plt.scatter(optimal_num_features, max_score, color='blue')\n",
    "plt.scatter(optimal_num_features_rf, max_score_rf, color='green')\n",
    "plt.annotate(f'Best XGBoost: {max_score:.4f}', (optimal_num_features, max_score), textcoords=\"offset points\", xytext=(-50,-15), ha='center', color='#377eb8')\n",
    "plt.annotate(f'Best RF: {max_score_rf:.4f}', (optimal_num_features_rf, max_score_rf), textcoords=\"offset points\", xytext=(10,8), ha='center', color='#4daf4a')\n",
    "\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns\n",
    "\n",
    "# Selected features\n",
    "selected_features = feature_names[rfecv_rd.support_]\n",
    "\n",
    "# Print selected features\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Parameters for the XGBoost model\n",
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 10,\n",
    "    \"n_estimators\": 300,\n",
    "    \"reg_alpha\": 1,\n",
    "    \"reg_lambda\": 10,\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": 3\n",
    "}\n",
    "\n",
    "num_simulations = 10\n",
    "\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "auc_pr_scores = []\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Simulation {i+1}\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "\n",
    "    precision, recall, f1, average_precision_recall_auc = metrics(y_test, y_pred, y_scores)\n",
    "\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    auc_pr_scores.append(average_precision_recall_auc)\n",
    "\n",
    "averaged_metrics = {\n",
    "    \"F1 Score\": f1_scores,\n",
    "    \"Precision\": precision_scores,\n",
    "    \"Recall\": recall_scores,\n",
    "    \"AUC-PR\": auc_pr_scores\n",
    "}\n",
    "\n",
    "averaged_results = {}\n",
    "\n",
    "for metric, scores in averaged_metrics.items():\n",
    "    average = np.mean(scores)\n",
    "    ci_lower, ci_upper = stats.t.interval(0.95, len(scores)-1, loc=average, scale=stats.sem(scores))\n",
    "    averaged_results[metric] = {\"Average\": average, \"95% CI\": (ci_lower, ci_upper)}\n",
    "\n",
    "averaged_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['psfMag_u', 'psfMag_g', 'psfMag_r', 'psfMag_i', 'psfMag_z',\n",
    "       'petroMag_r', 'petroMag_i', 'petroMag_z', 'psf_u-g', 'Target']]\n",
    "X_new = df_new.drop('Target', axis=1)\n",
    "y_new = df_new['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 10,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"min_samples_leaf\": 2\n",
    "}\n",
    "\n",
    "rf_f1_scores = []\n",
    "rf_precision_scores = []\n",
    "rf_recall_scores = []\n",
    "rf_auc_pr_scores = []\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    # Create and fit the model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.3)\n",
    "    rf_model = RandomForestClassifier(**rf_params)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    y_scores = rf_model.predict_proba(X_test)\n",
    "\n",
    "    precision, recall, f1, average_precision_recall_auc = metrics(y_test, y_pred, y_scores)\n",
    "\n",
    "    rf_f1_scores.append(f1)\n",
    "    rf_precision_scores.append(precision)\n",
    "    rf_recall_scores.append(recall)\n",
    "    rf_auc_pr_scores.append(average_precision_recall_auc)\n",
    "\n",
    "rf_averaged_metrics = {\n",
    "    \"F1 Score\": rf_f1_scores,\n",
    "    \"Precision\": rf_precision_scores,\n",
    "    \"Recall\": rf_recall_scores,\n",
    "    \"AUC-PR\": rf_auc_pr_scores\n",
    "}\n",
    "\n",
    "rf_averaged_results = {}\n",
    "\n",
    "for metric, scores in rf_averaged_metrics.items():\n",
    "    average = np.mean(scores)\n",
    "    ci_lower, ci_upper = stats.t.interval(0.95, len(scores)-1, loc=average, scale=stats.sem(scores))\n",
    "    rf_averaged_results[metric] = {\"Average\": average, \"95% CI\": (ci_lower, ci_upper)}\n",
    "\n",
    "rf_averaged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "num_simulations = 10\n",
    "n_classes = 3\n",
    "common_recall_thresholds = np.linspace(0, 1, 100) \n",
    "\n",
    "precision_dict = {i: [] for i in range(n_classes)}\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    y_test_binarized = label_binarize(y_test, classes=range(n_classes))\n",
    "\n",
    "    for class_id in range(n_classes):\n",
    "        precision, recall, _ = precision_recall_curve(y_test_binarized[:, class_id], y_scores[:, class_id])\n",
    "        precision_interp = np.interp(common_recall_thresholds, recall[::-1], precision[::-1])\n",
    "        precision_dict[class_id].append(precision_interp)\n",
    "\n",
    "avg_precision = {class_id: np.mean(precision_dict[class_id], axis=0) for class_id in range(n_classes)}\n",
    "\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(common_recall_thresholds, avg_precision[i], color=color, lw=2,\n",
    "             label=f'Class {i} (area = {auc(common_recall_thresholds, avg_precision[i]):0.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Average Precision-Recall Curve per Class')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import interpolate\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 10,\n",
    "    \"n_estimators\": 300,\n",
    "    \"reg_alpha\": 1,\n",
    "    \"reg_lambda\": 10,\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"num_class\": 3\n",
    "}\n",
    "\n",
    "num_simulations = 10\n",
    "n_classes = 3\n",
    "common_recall_thresholds = np.linspace(0, 1, 100) \n",
    "\n",
    "precision_dict = {i: [] for i in range(n_classes)}\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    y_test_binarized = label_binarize(y_test, classes=range(n_classes))\n",
    "\n",
    "    for class_id in range(n_classes):\n",
    "        precision, recall, _ = precision_recall_curve(y_test_binarized[:, class_id], y_scores[:, class_id])\n",
    "        precision_interp = np.interp(common_recall_thresholds, recall[::-1], precision[::-1])\n",
    "        precision_dict[class_id].append(precision_interp)\n",
    "\n",
    "avg_precision = {class_id: np.mean(precision_dict[class_id], axis=0) for class_id in range(n_classes)}\n",
    "\n",
    "target_colors = {0: '#377eb8', 1: '#4daf4a', 2: '#e41a1c'}  # blue, green, red\n",
    "class_names = {0: 'Star', 1: 'Galaxy', 2: 'Quasar'}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for class_id, color in target_colors.items():\n",
    "    plt.plot(common_recall_thresholds, avg_precision[class_id], color=color, lw=2,\n",
    "             label=f'{class_names[class_id]} (area = {auc(common_recall_thresholds, avg_precision[class_id]):0.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import interpolate\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": 10,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"min_samples_leaf\": 2\n",
    "}\n",
    "\n",
    "\n",
    "num_simulations = 5\n",
    "n_classes = 3\n",
    "common_recall_thresholds = np.linspace(0, 1, 100) \n",
    "\n",
    "precision_dict = {i: [] for i in range(n_classes)}\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.3)\n",
    "    model = RandomForestClassifier(**rf_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    y_test_binarized = label_binarize(y_test, classes=range(n_classes))\n",
    "\n",
    "    for class_id in range(n_classes):\n",
    "        precision, recall, _ = precision_recall_curve(y_test_binarized[:, class_id], y_scores[:, class_id])\n",
    "        precision_interp = np.interp(common_recall_thresholds, recall[::-1], precision[::-1])\n",
    "        precision_dict[class_id].append(precision_interp)\n",
    "\n",
    "avg_precision = {class_id: np.mean(precision_dict[class_id], axis=0) for class_id in range(n_classes)}\n",
    "\n",
    "target_colors = {0: '#377eb8', 1: '#4daf4a', 2: '#e41a1c'}  # blue, green, red\n",
    "class_names = {0: 'Star', 1: 'Galaxy', 2: 'Quasar'}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for class_id, color in target_colors.items():\n",
    "    plt.plot(common_recall_thresholds, avg_precision[class_id], color=color, lw=2,\n",
    "             label=f'{class_names[class_id]} (area = {auc(common_recall_thresholds, avg_precision[class_id]):0.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
